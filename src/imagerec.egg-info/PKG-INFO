Metadata-Version: 2.4
Name: imagerec
Version: 0.1.0
Summary: Image Recommender for Big Data Engineering course project
Requires-Python: >=3.9
Description-Content-Type: text/markdown

# Image Recommender (DAISY Big Data Engineering)

A local, modular image recommender designed for large datasets (~550k images). The codebase follows the course practices: generator-based data loading, SQLite for metadata/features, self-implemented similarity metrics, ANN search, and profiling hooks.

## Requirements
- macOS on Apple Silicon (M1)
- Python 3.9+

## Setup
1. Create and activate a virtual environment.
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Install the package (editable):
   ```bash
   pip install -e .
   ```
4. Update `config.yaml` with your dataset path and desired settings.

## CLI Commands
All commands run via:
```bash
python -m imagerec <command> [options]
```

### 1) Train / Fine-tune embeddings
```bash
python -m imagerec train-embeddings --epochs 10 --batch-size 64
```
- Uses contrastive learning with two augmented views of each image.
- Supports resumable training via checkpoints in `checkpoint_dir`.

### 2) Index dataset
```bash
python -m imagerec index
```
- Scans dataset with a generator, assigns IDs, stores metadata.
- Computes color histograms, perceptual hashes, and embeddings.

### 3) Build ANN index
```bash
python -m imagerec build-ann
```
- Builds an HNSW index over embeddings for fast top-k search.

### 4) Query similar images
```bash
python -m imagerec query --image /path/to/query.jpg --top-k 5
```
You can pass 2–3 images and combine metrics:
```bash
python -m imagerec query --image img1.jpg img2.jpg --top-k 5 --w-embedding 0.7 --w-color 0.2 --w-phash 0.1
```

### 5) Profiling
```bash
python -m imagerec profile --mode query --image /path/to/query.jpg
```
Generates `profile_query.prof` in `cache_dir` (use `snakeviz` or `pstats` to inspect).

### 6) 2D Visualization
```bash
python -m imagerec viz-2d --output ./data/embeddings_tsne.png --sample-size 2000
```

## Project Structure
```
.
├── config.yaml
├── requirements.txt
├── README.md
├── src/
│   └── imagerec/
│       ├── cli.py
│       ├── config.py
│       ├── db.py
│       ├── indexer.py
│       ├── query.py
│       ├── train.py
│       ├── ann.py
│       ├── profile.py
│       ├── viz.py
│       └── features/
└── tests/
```

## Design Notes
- **Generators / chunked iteration:** Dataset scanning (`iter_image_paths`) yields file paths lazily; indexing uses chunked batches to avoid loading all images into RAM.
- **SQLite schema:** `images` table links `image_id ↔ path ↔ metadata` and feature tables store color hist, embedding vectors, and perceptual hash values.
- **Similarity metrics:** Color histogram similarity (chi-square), embedding cosine similarity, and perceptual hash Hamming similarity are implemented manually in Python.
- **ANN search:** HNSW (via `hnswlib`) accelerates embedding search for top-5 results in seconds.

## Profiling & Optimization Notes
- `profile` command generates `.prof` files for indexing or querying.
- Typical hotspots: image decoding/resizing and embedding inference. If query latency is high, consider:
  - Increasing ANN efSearch in config
  - Using smaller `image_size`
  - Storing embeddings once and avoiding recomputation

## Scalability / Feasibility Discussion Hooks
- **Disk I/O:** Scanning and decoding dominates; SSD throughput is key. Generator-based loading keeps RAM bounded.
- **SQLite limits:** Works well for local, single-user indexing. For multi-user or very large features, a server DB is a future option.
- **ANN build time:** Linear in number of images; build once and reuse. Index size grows with embedding dimension.
- **Training:** Contrastive fine-tuning is feasible on M1 using MPS for small epochs. For larger datasets, run overnight or reduce epochs/augmentations.

## Running Tests
```bash
pytest
```
